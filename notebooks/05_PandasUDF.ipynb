{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8f0a138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34344e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .appName(\"FunctionsCh05\")\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b007f8",
   "metadata": {},
   "source": [
    "# User-Defined Functions\n",
    "Spark allows for engineers to build own functions aka User-Defined Functions (UDFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163e9e1c",
   "metadata": {},
   "source": [
    "## Spark SQL UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c263732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### First create a function\n",
    "def cubed(s):\n",
    "    return s * s * s    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac0fbe05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.cubed(s)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Then we'll register this UDF with the session. Will persist only for this session.\n",
    "spark.udf.register(\"cubed\", cubed, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "660c3937",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create temp view with range 1-9\n",
    "spark.range(1,9).createOrReplaceTempView(\"udf_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db147f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|id_cubed|\n",
      "+---+--------+\n",
      "|  1|       1|\n",
      "|  2|       8|\n",
      "|  3|      27|\n",
      "|  4|      64|\n",
      "|  5|     125|\n",
      "|  6|     216|\n",
      "|  7|     343|\n",
      "|  8|     512|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Now can query and even run UDF within a select statement.\n",
    "spark.sql(\"SELECT id, cubed(id) AS id_cubed FROM udf_test\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b1b8f8",
   "metadata": {},
   "source": [
    "## Pandas UDFs\n",
    "Downside to PySpark UDFs: Very expensive/slow b/c they run one row at a time. Resolved by introducing Pandas UDFs that run on Apache Arrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c41567ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "### Requirement: pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6494531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Again define our function\n",
    "def cubed(a: pd.Series) -> pd.Series:\n",
    "    return a * a * a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14712935",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create our pandas DF with our cubed function.\n",
    "cubed_udf = pandas_udf(cubed, returnType=LongType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1de5de",
   "metadata": {},
   "source": [
    "#### Here we can create a series with pandas and execute our function with that local pandas data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34d6381b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1\n",
      "1     8\n",
      "2    27\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "### Create the series\n",
    "x = pd.Series([1,2,3])\n",
    "\n",
    "### Execute our function\n",
    "print(cubed(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf901b8e",
   "metadata": {},
   "source": [
    "#### Now let's execute this function with a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40d4ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the series\n",
    "df = spark.range(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "586db83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|cubed(id)|\n",
      "+---+---------+\n",
      "|  1|        1|\n",
      "|  2|        8|\n",
      "|  3|       27|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Execute with Spark\n",
    "df.select(\"id\", cubed_udf(col(\"id\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3cd305",
   "metadata": {},
   "source": [
    "Can see with the Spark UI http://localhost:4040/jobs/ that we actually created and executed a spark job to run this calc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c022dac",
   "metadata": {},
   "source": [
    "# Higher-Order Functions\n",
    "Take anonymous lambda functions as arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7d65880",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create sample data set so we can run some examples\n",
    "schema = StructType([StructField(\"celsius\", ArrayType(IntegerType()))])\n",
    "\n",
    "t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]\n",
    "t_c = spark.createDataFrame(t_list, schema)\n",
    "t_c.createOrReplaceTempView(\"tC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85c46257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             celsius|\n",
      "+--------------------+\n",
      "|[35, 36, 32, 30, ...|\n",
      "|[31, 32, 34, 55, 56]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_c.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8753f8",
   "metadata": {},
   "source": [
    "### transform()\n",
    "Produces an array by applying a function to each element of the input array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f1f9193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             celsius|          fahrenheit|\n",
      "+--------------------+--------------------+\n",
      "|[35, 36, 32, 30, ...|[95, 96, 89, 86, ...|\n",
      "|[31, 32, 34, 55, 56]|[87, 89, 93, 131,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Calculate Fahrenheit from Celsius for an array of temperatures\n",
    "spark.sql(\"\"\"SELECT celsius,\n",
    "                transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit\n",
    "             FROM tC\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84733ff5",
   "metadata": {},
   "source": [
    "### filter()\n",
    "Produces an array consisting of only the elements of the input array for which the Boolean function is true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0486b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             celsius|    high|\n",
      "+--------------------+--------+\n",
      "|[35, 36, 32, 30, ...|[40, 42]|\n",
      "|[31, 32, 34, 55, 56]|[55, 56]|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Filter temperatures > 38C for array of temperatures\n",
    "spark.sql(\"\"\"SELECT celsius,\n",
    "                filter(celsius, t -> t > 38) as high\n",
    "             FROM tC\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e861cda2",
   "metadata": {},
   "source": [
    "### exists()\n",
    "Returns true if the Boolean function holds for any element in the input array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a10009ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             celsius|threshold|\n",
      "+--------------------+---------+\n",
      "|[35, 36, 32, 30, ...|     true|\n",
      "|[31, 32, 34, 55, 56]|    false|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Is there a temperature of 38C in the array of temperatures?\n",
    "spark.sql(\"\"\"SELECT celsius,\n",
    "                exists(celsius, t -> t = 38) as threshold\n",
    "             FROM tC\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97e071a",
   "metadata": {},
   "source": [
    "## aggregate()\n",
    "Reduces the elements of the array into a single value. Originally the reduce() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f323fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "### requirement\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "086bf6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|             celsius|avgFahrenheit|\n",
      "+--------------------+-------------+\n",
      "|[35, 36, 32, 30, ...|           96|\n",
      "|[31, 32, 34, 55, 56]|          105|\n",
      "+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Calculate average temperature and convert to F\n",
    "spark.sql(\"\"\"SELECT celsius,\n",
    "                aggregate(\n",
    "                celsius,\n",
    "                0,\n",
    "                (t, acc) -> t + acc,\n",
    "                acc -> (acc div size(celsius) * 9 div 5) + 32\n",
    "                ) as avgFahrenheit\n",
    "            FROM tC\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a3113e",
   "metadata": {},
   "source": [
    "# Common DataFrames and Spark SQL Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67a531d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tripdelaysFilePath = \"C:/Users/sean.cornillie/Education/LearningSparkV2/Spark_Dev/datasets/departuredelays.csv\"\n",
    "airportsnaFilePath = \"C:/Users/sean.cornillie/Education/LearningSparkV2/Spark_Dev/datasets/airport-codes-na.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a958dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "airportsna = (spark.read\n",
    "                  .format(\"csv\")\n",
    "                  .options(header=\"true\", inferSchema=\"true\", sep=\"\\t\")\n",
    "                  .load(airportsnaFilePath))\n",
    "\n",
    "airportsna.createOrReplaceTempView(\"airports_na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be2b7459",
   "metadata": {},
   "outputs": [],
   "source": [
    "departureDelays = (spark.read\n",
    "                      .format(\"csv\")\n",
    "                      .options(header=\"true\")\n",
    "                      .load(tripdelaysFilePath))\n",
    "\n",
    "departureDalays = (departureDelays\n",
    "                      .withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\"))\n",
    "                      .withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\")))\n",
    "\n",
    "departureDelays.createOrReplaceTempView(\"departureDelays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d711c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = (departureDelays\n",
    "          .filter(expr(\"\"\"origin == 'SEA'\n",
    "                          and destination == 'SFO'\n",
    "                          and date like '01010%'\n",
    "                          and delay > 0\"\"\")))\n",
    "\n",
    "foo.createOrReplaceTempView(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11e4fa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "+-----------+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM airports_na LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bf4598b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM departureDelays LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18013315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM foo\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcb8271",
   "metadata": {},
   "source": [
    "### Unions\n",
    "Can accomplish with both dataframe or sql syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7bbdc879",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = departureDelays.union(foo)\n",
    "bar.createOrReplaceTempView(\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1562d6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bar.filter(expr(\"\"\"origin == 'SEA'\n",
    "                    and destination == 'SFO'\n",
    "                    and date LIKE '01010%'\n",
    "                    and delay > 0\"\"\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ead645a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT *\n",
    "             FROM bar\n",
    "             WHERE 1=1\n",
    "                 and origin = 'SEA'\n",
    "                 and destination = 'SFO'\n",
    "                 and date LIKE '01010%'\n",
    "                 and delay > 0\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c68838",
   "metadata": {},
   "source": [
    "### Joins\n",
    "Again can accomplish with both df & sql syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4aee1a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo.join(airportsna, airportsna.IATA == foo.origin\n",
    "        ).select(\"City\", \"State\", \"date\", \"delay\", \"distance\", \"destination\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e44deda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT a.City, a.State, f.date, f.delay, f.distance, f.destination\n",
    "FROM foo f\n",
    "JOIN airports_na a\n",
    "ON a.IATA = f.origin\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a18d833",
   "metadata": {},
   "source": [
    "### Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c2afd2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+----+\n",
      "|    date|delay|distance|origin|destination|rank|\n",
      "+--------+-----+--------+------+-----------+----+\n",
      "|01261305|    9|     311|   BTM|        SLC|   1|\n",
      "|02011305|    9|     311|   BTM|        SLC|   1|\n",
      "|02062025|   99|     710|   BUR|        PDX|   1|\n",
      "|01051715|   98|    1064|   CAK|        DEN|   1|\n",
      "|01081337|   93|     139|   CDV|        ANC|   1|\n",
      "|01071710|   97|      48|   CEC|        ACV|   1|\n",
      "|01041225|   98|     397|   CHO|        ATL|   1|\n",
      "|02070921|   98|     602|   CID|        DEN|   1|\n",
      "|01070840|   99|     420|   CMH|        JFK|   1|\n",
      "|01161525|   99|     416|   CMH|        LGA|   1|\n",
      "+--------+-----+--------+------+-----------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT date\n",
    "                ,delay\n",
    "                ,distance\n",
    "                , origin\n",
    "                ,destination\n",
    "                ,rank\n",
    "             FROM (\n",
    "                 SELECT date\n",
    "                     ,delay\n",
    "                     ,distance\n",
    "                     ,origin\n",
    "                     ,destination\n",
    "                     ,dense_rank() OVER (\n",
    "                                     PARTITION BY origin\n",
    "                                     ORDER BY delay DESC) as rank\n",
    "                 FROM bar\n",
    "                 WHERE 1=1\n",
    "                     and delay > 0) t\n",
    "             WHERE 1=1\n",
    "                 and rank = 1\n",
    "                 \"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b43a012",
   "metadata": {},
   "source": [
    "### Modifications to DataFrames\n",
    "Remember that DFs are immutable, but we can create a new DF with changes that we need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a892fe50",
   "metadata": {},
   "source": [
    "#### Adding Columns\n",
    "Add a column designating on-time/delay status to foo table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39027ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+-------+\n",
      "|    date|delay|distance|origin|destination| status|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "|01010710|   31|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|  104|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|    5|     590|   SEA|        SFO|On-time|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo2 = (foo.withColumn(\n",
    "            \"status\",\n",
    "            expr(\"CASE WHEN delay <= 10 THEN 'On-time' ELSE 'Delayed' END\")\n",
    "        ))\n",
    "\n",
    "foo2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ffedd1",
   "metadata": {},
   "source": [
    "#### Dropping Columns\n",
    "No longer need delay length column, let's drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2b8ffd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------+\n",
      "|    date|distance|origin|destination| status|\n",
      "+--------+--------+------+-----------+-------+\n",
      "|01010710|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|     590|   SEA|        SFO|On-time|\n",
      "+--------+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo3 = foo2.drop(\"delay\")\n",
    "foo3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c20a9d",
   "metadata": {},
   "source": [
    "#### Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1999be87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------------+\n",
      "|    date|distance|origin|destination|flight_status|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "|01010710|     590|   SEA|        SFO|      Delayed|\n",
      "|01010955|     590|   SEA|        SFO|      Delayed|\n",
      "|01010730|     590|   SEA|        SFO|      On-time|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo4 = foo3.withColumnRenamed(\"status\", \"flight_status\")\n",
    "foo4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "86b17b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
