{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f8b2ad",
   "metadata": {},
   "source": [
    "# AirBnB Price Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44b57bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f960f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .appName(\"sf-airbnb\")\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38b47499",
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = \"C:/Users/sean.cornillie/Education/LearningSparkV2/Spark_Dev/datasets/sf-airbnb-clean.parquet/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e772faf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|neighbourhood_cleansed|      room_type|bedrooms|bathrooms|number_of_reviews|price|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|      Western Addition|Entire home/apt|     1.0|      1.0|            180.0|170.0|\n",
      "|        Bernal Heights|Entire home/apt|     2.0|      1.0|            111.0|235.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|             17.0| 65.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|              8.0| 65.0|\n",
      "|      Western Addition|Entire home/apt|     2.0|      1.5|             27.0|785.0|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airbnbDF = spark.read.parquet(filePath)\n",
    "\n",
    "airbnbDF.select(\"neighbourhood_cleansed\", \"room_type\", \"bedrooms\", \"bathrooms\", \"number_of_reviews\", \"price\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "428b2c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5780 rows in the training set, and 1366 in the test set\n"
     ]
    }
   ],
   "source": [
    "### We'll keep 80% of our data for the training set, and 20% for our test set.\n",
    "### Set random seed for reproducibility (if re-run with same random seed we should get same results)\n",
    "\n",
    "trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "print(f\"\"\"There are {trainDF.count()} rows in the training set, and {testDF.count()} in the test set\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5da9b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20781f19",
   "metadata": {},
   "source": [
    "#### Prepping features with Transformers\n",
    "Here we'll prep the data to build a linear regression model predicting price given a # of bedrooms. V simple example, classic y=mx+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6776bc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+\n",
      "|bedrooms|features|price|\n",
      "+--------+--------+-----+\n",
      "|     1.0|   [1.0]|200.0|\n",
      "|     1.0|   [1.0]|130.0|\n",
      "|     1.0|   [1.0]| 95.0|\n",
      "|     1.0|   [1.0]|250.0|\n",
      "|     3.0|   [3.0]|250.0|\n",
      "|     1.0|   [1.0]|115.0|\n",
      "|     1.0|   [1.0]|105.0|\n",
      "|     1.0|   [1.0]| 86.0|\n",
      "|     1.0|   [1.0]|100.0|\n",
      "|     2.0|   [2.0]|220.0|\n",
      "+--------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Using VectorAssembler, we are prepping/transforming our data into a format that our linear regression model expects\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=[\"bedrooms\"], outputCol=\"features\")\n",
    "vecTrainDF = vecAssembler.transform(trainDF)\n",
    "vecTrainDF.select(\"bedrooms\", \"features\", \"price\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbdaf43",
   "metadata": {},
   "source": [
    "#### Using Estimators to build models\n",
    "Here we actually build our model to estimated price per bedroom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9bf448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ac1e588",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "lrModel = lr.fit(vecTrainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a013028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The formula for the linear regression line is price = 123.6757463819947*bedrooms + 47.51023373378815\n"
     ]
    }
   ],
   "source": [
    "### Output our coefficients and organize locially below\n",
    "\n",
    "m = lrModel.coefficients[0]\n",
    "b = lrModel.intercept\n",
    "\n",
    "print(f\"\"\"The formula for the linear regression line is price = {m}*bedrooms + {b}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a062c8e1",
   "metadata": {},
   "source": [
    "#### Creating a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3951dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bebc7bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prep a pipeline that we can simply just apply to our test data\n",
    "pipeline = Pipeline(stages=[vecAssembler, lr])\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a784a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+------------------+\n",
      "|bedrooms|features| price|        prediction|\n",
      "+--------+--------+------+------------------+\n",
      "|     1.0|   [1.0]|  85.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  45.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  70.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 128.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 159.0|171.18598011578285|\n",
      "|     2.0|   [2.0]| 250.0|294.86172649777757|\n",
      "|     1.0|   [1.0]|  99.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  95.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 100.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|2010.0|171.18598011578285|\n",
      "+--------+--------+------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Apply pipeline to our test data\n",
    "### Very simple model that doesn't provide great predictions against results\n",
    "\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"bedrooms\", \"features\", \"price\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9386a81",
   "metadata": {},
   "source": [
    "#### One-hot encoding\n",
    "Let's build a slightly more complex pipeline that incorporates all of the dataset's numeric & categorical features. <br/>\n",
    "Most machine learning models in MLib expect numerical values as input, represented as vectors. <br/>\n",
    "So, we'll use one-hot encoding (OHE) to convert categoricals into numerics. <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21b8b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we have a column called Animal and have three types\n",
    "### We can't pass string types into our ML model so need at assign numeric mapping, such as:\n",
    "#### Animal = {\"Dog\", \"Cat\", \"Fish\"}\n",
    "#### \"Dog\" = 1, \"Cat\" = 2, \"Fish\" = 3\n",
    "\n",
    "# However this approach could introduce some relationships that don't exist. E.g. is cat worth 2 dogs, etc?\n",
    "### So, we'll use OHE to vectorize existance of dog/cat/fish in our animal column.\n",
    "#### \"Dog\" =  [ 1, 0, 0]\n",
    "#### \"Cat\" =  [ 0, 1, 0]\n",
    "#### \"Fish\" = [ 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "307abec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6c0e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical columns\n",
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes if dataType == \"string\"]\n",
    "\n",
    "\n",
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "oheOutputCols = [x + \"OHE\" for x in categoricalCols]\n",
    "\n",
    "# Create indexes of all strings in each categorical column\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols,\n",
    "                              outputCols=indexOutputCols,\n",
    "                              handleInvalid=\"skip\")\n",
    "\n",
    "# Encodes those indexes into column of binary vectors\n",
    "oheEncoder = OneHotEncoder(inputCols=indexOutputCols,\n",
    "                           outputCols=oheOutputCols)\n",
    "\n",
    "#Define numeric columns \n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes if ((dataType == \"double\") & (field != \"price\"))]\n",
    "\n",
    "#Re-assemble converted OHE & Numeric columns\n",
    "assemblerInputs = oheOutputCols + numericCols\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs,\n",
    "                               outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1003893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another (less complicated) way to do this is to use RFormula.\n",
    "# RFormula automatically StringIndex & OHE all of your string columns, convert numerics to double type, and combine into single vector using VectorAssembler under the hood.\n",
    "# All we need to do is provide it with a formula = \"y ~ bedrooms + bathrooms\"\n",
    "### In the case below \"price ~ .\", we're predicting price based on all (.) features.\n",
    "# Thus, we could replace all of the above code with the snippet below\n",
    "\n",
    "from pyspark.ml.feature import RFormula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9161d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "rFormula = RFormula(formula = \"price ~ .\",\n",
    "                    featuresCol=\"features\",\n",
    "                    labelCol=\"price\",\n",
    "                    handleInvalid=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14353eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|price|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(98,[0,3,6,22,43,...| 85.0| 55.24365707389188|\n",
      "|(98,[0,3,6,22,43,...| 45.0|23.357685914717877|\n",
      "|(98,[0,3,6,22,43,...| 70.0|28.474464479034395|\n",
      "|(98,[0,3,6,12,42,...|128.0| -91.6079079594947|\n",
      "|(98,[0,3,6,12,43,...|159.0| 95.05688229945372|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(labelCol=\"price\", featuresCol=\"features\")\n",
    "pipeline = Pipeline(stages = [stringIndexer, oheEncoder, vecAssembler, lr])\n",
    "### Alt pipeline: use RFormula ###\n",
    "#pipeline = Pipeline(stages = [rFormula, lr])\n",
    "\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"features\", \"price\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868da235",
   "metadata": {},
   "source": [
    "### Evaluating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360d4e63",
   "metadata": {},
   "source": [
    "#### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9378f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "94878570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 220.6\n"
     ]
    }
   ],
   "source": [
    "regressionEvaluator = RegressionEvaluator(\n",
    "    predictionCol=\"prediction\",\n",
    "    labelCol=\"price\",\n",
    "    metricName=\"rmse\")\n",
    "\n",
    "rmse = regressionEvaluator.evaluate(predDF)\n",
    "print(f\"RMSE is {rmse:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5869a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.56321700343753\n"
     ]
    }
   ],
   "source": [
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3945e71",
   "metadata": {},
   "source": [
    "#### R-Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ed877ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16043316698848087\n"
     ]
    }
   ],
   "source": [
    "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a75627",
   "metadata": {},
   "source": [
    "### Lab/HW: Re-train & fit model with Log prices\n",
    "This R2 very bad. Chiefly b/c ML models will fare better with a log-normal distribution, and we did not actually convert to begin with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96f03075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5780 rows in the training set, and 1366 in the test set\n"
     ]
    }
   ],
   "source": [
    "filePath = \"C:/Users/sean.cornillie/Education/LearningSparkV2/Spark_Dev/datasets/sf-airbnb-clean.parquet/\"\n",
    "airbnbDF = spark.read.parquet(filePath)\n",
    "trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "print(f\"\"\"There are {trainDF.count()} rows in the training set, and {testDF.count()} in the test set\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "db05cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logTrainDF = trainDF.withColumn(\"log_price\", log(col(\"price\")))\n",
    "logTestDF = trainDF.withColumn(\"log_price\", log(col(\"price\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a112259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rFormula = RFormula(formula = \"log_price ~ . - price\", ## log_price based on all features except price\n",
    "                    featuresCol=\"features\",\n",
    "                    labelCol=\"log_price\",\n",
    "                    handleInvalid=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0271e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(labelCol=\"log_price\", predictionCol=\"log_pred\")\n",
    "pipeline = Pipeline(stages = [rFormula, lr])\n",
    "\n",
    "pipelineModel = pipeline.fit(logTrainDF)\n",
    "predDF = pipelineModel.transform(logTestDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c848caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "expDF = predDF.withColumn(\"prediction\", exp(col(\"log_pred\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "81c1cae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 288.05105439893686\n",
      "R2 is 0.2184102251246367\n"
     ]
    }
   ],
   "source": [
    "regressionEvaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n",
    "rmse = regressionEvaluator.setMetricName(\"rmse\").evaluate(expDF)\n",
    "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(expDF)\n",
    "\n",
    "print(f\"RMSE is {rmse}\")\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b29a9",
   "metadata": {},
   "source": [
    "#### Now let's save our pipeline model for future re-use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eb9f9ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinePath = \"/tmp/lr-pipeline-model\"\n",
    "pipelineModel.write().overwrite().save(pipelinePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0b6fafc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
